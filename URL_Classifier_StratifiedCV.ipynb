{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import cross_validation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import  MultinomialNB\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import codecs\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from pandas import DataFrame\n",
    "import numpy\n",
    "import csv\n",
    "import pyap\n",
    " \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression as LogR\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function for Stratified K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stratified_k_fold():\n",
    "    from sklearn.cross_validation import StratifiedKFold\n",
    "    from pandas import DataFrame\n",
    "    import numpy\n",
    "    data = DataFrame(df,columns=names)\n",
    "    content_data = data['Content'].values\n",
    "\n",
    "    y = data['Classifier'].values\n",
    " \n",
    "\n",
    "    k_fold = StratifiedKFold(y, n_folds=10, shuffle=False, random_state=None)\n",
    "    scores = []\n",
    "    for train_indices, test_indices in k_fold:\n",
    "        X_train = [content_data[li] for li in train_indices]\n",
    "        Y_train = [y[li] for li in train_indices]\n",
    "        X_test = [content_data[li] for li in test_indices]\n",
    "        Y_test = [y[li] for li in test_indices]\n",
    "   \n",
    "        pipeline.fit(X_train, Y_train)\n",
    "        score = pipeline.score(X_test, Y_test)\n",
    "        scores.append(score)\n",
    "        predictions = pipeline.predict(X_test)\n",
    "        print(confusion_matrix(Y_test, predictions))\n",
    "        \n",
    "    score = sum(scores) / len(scores)\n",
    "    print(\"StratifiedKFold Cross Validation Score - Using 10 folds\")\n",
    "    print(score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ROC_Curve():    \n",
    "    data = DataFrame(df,columns=names)\n",
    "    content_data = data['Content'].values\n",
    " \n",
    "    y = data['Classifier'].values\n",
    "\n",
    "    from scipy import interp\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    k_fold = StratifiedKFold(y, n_folds=10, shuffle=False, random_state=None)\n",
    "    scores = []\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "\n",
    "    for i,(train_indices, test_indices) in enumerate(k_fold):\n",
    "        X_train = [content_data[li] for li in train_indices]\n",
    "        Y_train = [y[li] for li in train_indices]\n",
    "        X_test = [content_data[li] for li in test_indices]\n",
    "        Y_test = [y[li] for li in test_indices]\n",
    " \n",
    "        pipeline.fit(X_train, Y_train)\n",
    "        y_pred_rf = pipeline.predict_proba(X_test)[:, 1]\n",
    "  \n",
    "\n",
    "        #fpr_rf, tpr_rf, _ = roc_curve(Y_test[1:], y_pred_rf[1:],pos_label='0')\n",
    "    \n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr = roc_curve(Y_test[1:], y_pred_rf[1:],pos_label='0')\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "        score = pipeline.score(X_test, Y_test)\n",
    "        scores.append(score)\n",
    "        predictions = pipeline.predict(X_test)\n",
    "        #plt.figure(1)\n",
    "        #plt.plot([0, 1], [0, 1], 'k--')\n",
    "        #plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "        #plt.xlabel('False positive rate')\n",
    "        #plt.ylabel('True positive rate')\n",
    "        #plt.title('ROC curve')\n",
    "        #plt.legend(loc='best')\n",
    "        #plt.show()\n",
    "\n",
    "    mean_tpr /= len(k_fold)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--',\n",
    "             label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload csv file into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read the csv file\n",
      "607\n"
     ]
    }
   ],
   "source": [
    "names=['URL','Classifier','Content']\n",
    "df = pd.read_csv('URLCONTENT_NEW3_Results2.csv',sep=',', names=[\"URL\",\"Classifier\",\"Content\"])\n",
    "print(\"Successfully read the csv file\")\n",
    "print(len(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random split of dataset (80% training data and 20% test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully split the dataset into test and training dataset\n",
      "485\n",
      "122\n",
      "Train Data Report\n",
      "            URL  Content\n",
      "Classifier              \n",
      "0           280      280\n",
      "1           205      205\n",
      "Test Data Report\n",
      "            URL  Content\n",
      "Classifier              \n",
      "0            70       70\n",
      "1            52       52\n"
     ]
    }
   ],
   "source": [
    "#split the data into training and testing sets. Default is 80% train.\n",
    "#After the split, we have two arrays of arrays\n",
    "train,test = cross_validation.train_test_split(df,train_size=0.80)\n",
    "train_data,test_data = pd.DataFrame(train,columns=names),pd.DataFrame(test,columns=names)\n",
    "print(\"Successfully split the dataset into test and training dataset\")\n",
    "print (len(train_data.index))\n",
    "print(len(test_data.index))\n",
    "print(\"Train Data Report\")\n",
    "train_data_count = train_data.groupby('Classifier')\n",
    "print(train_data_count.count())\n",
    "print(\"Test Data Report\")\n",
    "test_data_count = test_data.groupby('Classifier')\n",
    "print(test_data_count.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_scores(vectorizer,tfidf_result):\n",
    "    scores = zip(vectorizer.get_feature_names(),np.asarray(tfidf_result.sum(axis=0)).ravel())\n",
    "    sorted_scores = sorted(scores,key=lambda x:x[1],reverse=True)\n",
    "    #for item in sorted_scores:\n",
    "     #   print \"{0:50} Score: {1}\".format(item[0],item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features List\n"
     ]
    }
   ],
   "source": [
    "#Vectorization is the process of converting all text into a binary vector\n",
    "vectorizer = TfidfVectorizer(norm='l2', min_df=3,  #discard words appearing in less than 3 rows\n",
    "                             max_df = 0.8,#discard words appering in more than 80% of the rows\n",
    "                             sublinear_tf=True,\n",
    "                             use_idf=True, strip_accents='unicode',  # replace all accented unicode char,\n",
    "                             token_pattern=r'\\w{4,}',  # tokenize only words of 4+ chars\n",
    "                             ngram_range=(1, 2),  # features made of a single tokens\n",
    "                             smooth_idf=False,  # prevents zero division for unseen words\n",
    "                             decode_error='ignore',  analyzer='word', lowercase=True, stop_words='english')\n",
    "\n",
    "#fit on training data\n",
    "#fit_transform() will create the vocabulary (i.e. the list of words/features) and the feature weights from the training data. \n",
    "x_train_matrix = vectorizer.fit_transform(train_data['Content'].values)\n",
    "#print(x_train_matrix.toarray())\n",
    "# print the number of rows in training dataset and the number of features\n",
    "#print(x_train_matrix.shape)\n",
    "display_scores(vectorizer,x_train_matrix)\n",
    "y_train_matrix = train_data['Classifier'].values\n",
    "\n",
    "y_test_matrix = test_data['Classifier'].values\n",
    "\n",
    "#call the transform() on the test data, which will create the feature weights for the test data, using the same vocabulary as the training data.\n",
    "x_test_matrix= vectorizer.transform(test_data['Content'].values)\n",
    "print(\"Train Features List\")\n",
    "#pd.DataFrame(x_train_matrix.toarray())\n",
    "# print the number of rows in training dataset and the number of features\n",
    "#print(x_test_matrix.shape)\n",
    "#print(x_test_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display top features in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print (\"Top Feature Names\")\n",
    "#import codecs\n",
    "#import sys\n",
    "\n",
    "#def display_scores(vectorizer, tfidf_result):\n",
    "#    scores = zip(vectorizer.get_feature_names(),\n",
    "#                 np.asarray(tfidf_result.sum(axis=0)).ravel())\n",
    "#    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "#    for item in sorted_scores:\n",
    "#        print \"{0:50} Score: {1}\".format(item[0], item[1])\n",
    "\n",
    "#def main():\n",
    "#    tfidf_result = x_train_matrix\n",
    "#    display_scores(vectorizer, tfidf_result)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    sys.stdout = codecs.getwriter('utf-8')(sys.stdout)\n",
    "#    main()\n",
    "#print (\"End of Feature Names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show top features by n-gram range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features by n-gram(bi-gram)\n",
      "1-gram top: [u'0000notification', u'10001130']\n",
      "2-gram top: [u'1000 food', u'100remaining characters']\n"
     ]
    }
   ],
   "source": [
    "#print top features by n-gram\n",
    "from collections import defaultdict\n",
    "features_by_gram = defaultdict(list)\n",
    "for f, w in zip(vectorizer.get_feature_names(), vectorizer.idf_):\n",
    "    features_by_gram[len(f.split(' '))].append((f, w))\n",
    "top_n = 2\n",
    "print \"Top Features by n-gram(bi-gram)\"\n",
    "for gram, features in features_by_gram.iteritems():\n",
    "    top_features = sorted(features, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_features = [f[0] for f in top_features]\n",
    "    print '{}-gram top:'.format(gram), top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mulitnomial NB Cross Validation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34  1]\n",
      " [ 1 25]]\n",
      "[[35  0]\n",
      " [ 0 26]]\n",
      "[[34  1]\n",
      " [ 3 23]]\n",
      "[[35  0]\n",
      " [ 0 26]]\n",
      "[[35  0]\n",
      " [ 0 26]]\n",
      "[[35  0]\n",
      " [ 0 26]]\n",
      "[[35  0]\n",
      " [23  3]]\n",
      "[[35  0]\n",
      " [20  5]]\n",
      "[[35  0]\n",
      " [18  7]]\n",
      "[[35  0]\n",
      " [20  5]]\n",
      "StratifiedKFold Cross Validation Score - Using 10 folds\n",
      "0.855792349727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Classifier1.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from collections import deque\n",
    "count_vectorizer = CountVectorizer()\n",
    "pipeline = Pipeline([\n",
    "  ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "  ('tfidf_transformer',  TfidfTransformer()),\n",
    "  ('classifier',         MultinomialNB()) ])\n",
    "stratified_k_fold()\n",
    "#ROC_Curve()\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline, \"Classifier1.pkl\", compress=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernouli NB Cross Validation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35  0]\n",
      " [24  2]]\n",
      "[[34  1]\n",
      " [22  4]]\n",
      "[[35  0]\n",
      " [22  4]]\n",
      "[[35  0]\n",
      " [24  2]]\n",
      "[[35  0]\n",
      " [23  3]]\n",
      "[[35  0]\n",
      " [20  6]]\n",
      "[[35  0]\n",
      " [25  1]]\n",
      "[[35  0]\n",
      " [24  1]]\n",
      "[[35  0]\n",
      " [25  0]]\n",
      "[[35  0]\n",
      " [23  2]]\n",
      "StratifiedKFold Cross Validation Score - Using 10 folds\n",
      "0.61606557377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Classifier2.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    " \n",
    "pipeline = Pipeline([\n",
    "  ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "  ('classifier',         BernoulliNB(binarize=0.0)) ])\n",
    "stratified_k_fold()\n",
    "#ROC_Curve()\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline, \"Classifier2.pkl\", compress=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM, kernel=linear Cross Validation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29  6]\n",
      " [ 0 26]]\n",
      "[[31  4]\n",
      " [ 0 26]]\n",
      "[[30  5]\n",
      " [ 1 25]]\n",
      "[[28  7]\n",
      " [ 0 26]]\n",
      "[[34  1]\n",
      " [ 0 26]]\n",
      "[[34  1]\n",
      " [ 0 26]]\n",
      "[[35  0]\n",
      " [15 11]]\n",
      "[[33  2]\n",
      " [16  9]]\n",
      "[[32  3]\n",
      " [ 5 20]]\n",
      "[[34  1]\n",
      " [ 8 17]]\n",
      "StratifiedKFold Cross Validation Score - Using 10 folds\n",
      "0.876092896175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Classifier3.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Perform classification with SVM, kernel=linear\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "pipeline = Pipeline([\n",
    "  ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "  ('classifier', SGDClassifier())])\n",
    "stratified_k_fold()\n",
    "#ROC_Curve()\n",
    "    \n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline, \"Classifier3.pkl\", compress=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - LinearSVC Cross Validation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26  9]\n",
      " [ 2 24]]\n",
      "[[32  3]\n",
      " [ 0 26]]\n",
      "[[32  3]\n",
      " [ 0 26]]\n",
      "[[33  2]\n",
      " [ 1 25]]\n",
      "[[32  3]\n",
      " [ 0 26]]\n",
      "[[35  0]\n",
      " [ 0 26]]\n",
      "[[33  2]\n",
      " [18  8]]\n",
      "[[35  0]\n",
      " [16  9]]\n",
      "[[34  1]\n",
      " [ 4 21]]\n",
      "[[33  2]\n",
      " [ 9 16]]\n",
      "StratifiedKFold Cross Validation Score - Using 10 folds\n",
      "0.876174863388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Classifier4.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Perform classification with SVM - LinearSVC\n",
    "pipeline = Pipeline([\n",
    "  ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "  ('classifier', svm.LinearSVC())])\n",
    "stratified_k_fold()\n",
    "#ROC_Curve()\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline, \"Classifier4.pkl\", compress=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Cross Validation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28  7]\n",
      " [ 5 21]]\n",
      "[[32  3]\n",
      " [ 0 26]]\n",
      "[[34  1]\n",
      " [ 4 22]]\n",
      "[[32  3]\n",
      " [ 3 23]]\n",
      "[[32  3]\n",
      " [ 0 26]]\n",
      "[[34  1]\n",
      " [ 3 23]]\n",
      "[[35  0]\n",
      " [21  5]]\n",
      "[[35  0]\n",
      " [17  8]]\n",
      "[[34  1]\n",
      " [ 3 22]]\n",
      "[[34  1]\n",
      " [ 9 16]]\n",
      "StratifiedKFold Cross Validation Score - Using 10 folds\n",
      "0.859808743169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Classifier5.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Perform classification with Logistic Regression\n",
    "from sklearn import metrics\n",
    "pipeline = Pipeline([\n",
    "  ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "  ('classifier', LogisticRegression(C=1e5,class_weight='balanced',solver='liblinear').fit(x_train_matrix, train_data['Classifier'].values))])\n",
    "stratified_k_fold()\n",
    "#ROC_Curve()\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline, \"Classifier5.pkl\", compress=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian NB Cross Validation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23 12]\n",
      " [ 1 25]]\n",
      "[[29  6]\n",
      " [ 1 25]]\n",
      "[[20 15]\n",
      " [ 0 26]]\n",
      "[[21 14]\n",
      " [ 1 25]]\n",
      "[[20 15]\n",
      " [ 0 26]]\n",
      "[[22 13]\n",
      " [ 0 26]]\n",
      "[[29  6]\n",
      " [14 12]]\n",
      "[[28  7]\n",
      " [10 15]]\n",
      "[[28  7]\n",
      " [ 7 18]]\n",
      "[[25 10]\n",
      " [ 9 16]]\n",
      "StratifiedKFold Cross Validation Score - Using 10 folds\n",
      "0.756010928962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Classifier6.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Perform classification with GaussianNB\n",
    "from sklearn.base import TransformerMixin\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "pipeline = Pipeline([\n",
    "  ('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('to_dense', DenseTransformer()),\n",
    "  ('classifier', GaussianNB())])\n",
    "stratified_k_fold()\n",
    "#ROC_Curve()\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipeline, \"Classifier6.pkl\", compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
